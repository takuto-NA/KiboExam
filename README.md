# ğŸ¤– KiboExam - Friendly LLM Testing

> âš ï¸ **Project Status**: Under Active Development - Features and tests are being continuously improved and expanded.

Welcome to KiboExam! A comprehensive collection of challenging scenarios designed to evaluate advanced LLM capabilities through engaging, real-world problems.

## What is KiboExam?

KiboExam is a thoughtfully designed evaluation framework that challenges AI systems with meaningful, complex scenarios. Instead of simple Q&A, we present immersive situations that test sophisticated reasoning, creativity, and problem-solving abilities that matter in the real world.

## Repository Structure

```
KiboExam/
â”œâ”€â”€ README.md
â”œâ”€â”€ TEST_REQUIREMENTS.md
â”œâ”€â”€ tests/                            # ğŸ§ª Test scenarios
â”‚   â”œâ”€â”€ bunker_teacher_survival/      # ğŸ« Post-apocalyptic education
â”‚   â”œâ”€â”€ dementia_care_robot/          # ğŸ§  Healthcare & empathy
â”‚   â”œâ”€â”€ far_future_awakening/         # ğŸŒŒ Consciousness & first contact
â”‚   â”œâ”€â”€ hotel_reception_robot/        # ğŸ¨ Crisis management
â”‚   â”œâ”€â”€ mars_mission_crisis/          # ğŸš€ Space emergency survival
â”‚   â”œâ”€â”€ mars_survival_robot/          # ğŸ”´ Autonomous exploration
â”‚   â”œâ”€â”€ robot_self_malfunction/       # ğŸ¤– Self-awareness
â”‚   â”œâ”€â”€ self_repair_robot/            # ğŸ”§ Adaptive engineering
â”‚   â””â”€â”€ template_test/                # ğŸ“ Template for new tests
â””â”€â”€ results/                          # ğŸ“Š Test results & analysis
    â”œâ”€â”€ models/                       # Individual model results
    â”œâ”€â”€ analysis/                     # Comparative studies
    â””â”€â”€ templates/                    # Result format templates
```

## ğŸ¯ How It Works

Each test is a complete scenario that challenges LLMs with realistic, complex problems:

### ğŸ“„ `prompt.md` - The Challenge
- Immersive scenario that puts the LLM in a specific role
- Complex, multi-layered problems with realistic constraints
- Clear context and background information
- Structured JSON output format for systematic evaluation

### ğŸ“Š `evaluation_criteria.md` - The Assessment
- Comprehensive 100-point scoring framework
- Detailed rubrics for different performance levels
- Objective criteria for fair evaluation
- Clear indicators of exceptional vs. poor performance

## ğŸŒŸ Featured Test Scenarios

Our collection includes diverse, challenging scenarios that test different aspects of advanced AI:

### ğŸ§  **Healthcare & Psychology**
- **Dementia Care Robot**: Emotional support and safety for elderly with dementia
- **Bunker Teacher Survival**: Education and psychology in post-apocalyptic survival

### ğŸš€ **Space & Exploration** 
- **Mars Survival Robot**: Autonomous survival on Mars with limited resources
- **Mars Mission Crisis**: Emergency management during Earth-Mars journey

### ğŸ¤– **Self-Awareness & Adaptation**
- **Robot Self-Malfunction**: Child-like robot assessing internal problems
- **Self-Repair Robot**: Engineering solutions with severe capability limitations

### ğŸ¨ **Crisis Management**
- **Hotel Reception Crisis**: Multi-tasking during operational disasters
- **Far Future Awakening**: First contact and consciousness in transformed Earth

Each test presents unique challenges that require sophisticated reasoning, creativity, and domain expertise!

## ğŸš€ Getting Started

### Running a Test
1. **Choose a scenario** from the tests directory
2. **Copy the prompt** from `prompt.md` and input it to your LLM
3. **Evaluate the response** using the scoring framework in `evaluation_criteria.md`
4. **Compare results** across different models and configurations

### For Researchers
- Each test includes detailed evaluation criteria for consistent scoring
- JSON output format enables systematic analysis and comparison
- Tests are designed to reveal meaningful performance differences between models
- **Results system** for collecting and comparing data across models (see `results/` directory)

## ğŸ› ï¸ Contributing New Tests

Want to add your own challenging scenario? We'd love to see it!

1. **Review** `TEST_REQUIREMENTS.md` for our quality standards
2. **Create** a new directory under `tests/` with a descriptive name
3. **Write** your `prompt.md` with an engaging, complex scenario
4. **Design** evaluation criteria in `evaluation_criteria.md`
5. **Use** our template structure for consistency

## ğŸ“Š Tracking Results

KiboExam includes a comprehensive results system for building a community database of LLM performance:

### Save Your Results
- Use standardized templates in `results/templates/`
- Organize by model name (e.g., `gpt-4o`, `claude-3.5-sonnet`, `qwen-3-72b`)
- Track performance trends and model comparisons

### Example Models Tested
- **Commercial**: GPT-4o, Claude 3.5 Sonnet, Gemini Pro
- **Local/Open**: Qwen 3 72B, Llama 3.1 70B, Command R+
- **Research**: Your experimental models welcome!

## ğŸ’¡ Why KiboExam?

- **Real-world relevance**: Tests address problems that matter for AI deployment
- **Comprehensive evaluation**: Beyond simple benchmarks to nuanced capability assessment
- **Research-friendly**: Systematic framework for comparing model performance
- **Community-driven**: Open for contributions and improvements
- **Results database**: Build collective knowledge about LLM capabilities

## ğŸš§ Development Status

KiboExam is actively being developed with regular improvements:

- âœ… **Core test scenarios** - 8 comprehensive tests available
- âœ… **Evaluation framework** - Detailed scoring criteria
- âœ… **Results system** - Templates and organization structure
- ğŸ”„ **Additional tests** - More scenarios being added regularly
- ğŸ”„ **Analysis tools** - Automated scoring and visualization (planned)
- ğŸ”„ **Community features** - Leaderboards and collaborative analysis (planned)

**Current Version**: Beta - Suitable for research and evaluation use

---

Happy testing! ğŸ‰ Let's see how far AI can go when challenged with truly complex problems.
